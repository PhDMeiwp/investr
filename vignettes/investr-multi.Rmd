---
title: "Inverse Estimation (Multiple Predictor Case)"
author: "Brandon M. Greenwell"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

As of version 1.4.0 multiple predictor variables are allowed for objects of class `"lm"` and `"glm"`; that is, both linear and generalized linear models, respectively.

Suppose, for example, that for a fitted mean response of the form
$$
\widehat{Y} = \widehat{\beta}_0 + \sum_{i=1}^p \widehat{\beta}_i x_i, \quad i = 1, 2, \dots, n,
$$
it is desired to estimate the value of a specific $x_j$ for a given mean response, $m_0$ say, while all the other predictors ($x_i$, $i \neq j$) are held at some fixed value. For the most part, the methodology is the same as for inverse estimation with a single predictor variable - we simply solve for the unknown $x_j$. Standard errors and confidence intervals also follow in a similar fashion. For details, the interested reader is pointed to Draper and Smith (1998, pp. 229 - 230).

### Inverse estimation with multiple predictor variables

For illustration, we'll simulate some data according to the model
$$
Y = 1 + 2x_1 - 3x_2 + x_3 + \epsilon,
$$
where $\epsilon \sim \mathcal{N}\left(0, 0.1^2\right)$. The data generation is carried out in the following snippet of code.
```{r}
set.seed(101) # for reproducibility
x <- matrix(rnorm(300), nrow = 100, ncol = 3)
y <- 1 + 2*x[, 1] - 3*x[, 2] + x[, 3] + rnorm(100, sd = 0.1)
sim <- data.frame(x, y)  # make into a data frame
names(sim) <- c("x1", "x2", "x3", "y")  # add better column names
pairs(y ~ ., data = sim, col = "purple")  # simple scatterplot matrix
```

Next, we'll fit a linear model to the simulated data `sim`.
```{r}
fm <- lm(y ~ ., data = sim)  # fit a linear model
summary(fm)  # coefficients, standard errors, etc.
```

Suppose we want to estimate the value of $x_2$ corresponding to a mean response of $E\left[Y\right] = 0$ when $x_1$ and $x_2$ are also both held at zero. The true value of $x_2$ is then given by
$$
(y_0 - \beta_0 - \beta_1 x_1 - \beta_3 x_3) / \beta_2 = (0 - 1) / (-3) = 1/3.
$$
```{r}
# Required packages
library(investr)

# Inverse estimation
invest(fm, y0 = 0, x0.name = "x2", newdata = data.frame(x1 = 0, x3 = 0))

# Compare to
(0 - 1) / (-3)
```

### Factors

An example of inverse estimation is given in Venables, W. N. & Ripley, B. D. (2002). The specific example (see `?MASS::dose.p`) is re-created in the following snippet of code.
```{r}
# Load package, assuming it is already installed
library(MASS)

# Data
ldose <- rep(0:5, 2)
numdead <- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex <- factor(rep(c("M", "F"), c(6, 6)))
SF <- cbind(numdead, numalive = 20 - numdead)
budworm <- data.frame(ldose, numdead, sex, SF)

# Logistic regression
budworm.lg0 <- glm(SF ~ sex + ldose - 1, family = binomial, data = budworm)

# Using dose.p function from package MASS
dose.p(budworm.lg0, cf = c(1, 3), p = 1/4)

# Using invest function from package investr (Wald interval)
invest(budworm.lg0, y0 = 1/4, 
       interval = "Wald",
       x0.name = "ldose", 
       newdata = data.frame(sex = "F"))

# Using invest function from package investr (inversion interval)
invest(budworm.lg0, y0 = 1/4, 
       interval = "inversion",
       x0.name = "ldose", 
       newdata = data.frame(sex = "F"))
```